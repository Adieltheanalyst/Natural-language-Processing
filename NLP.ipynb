{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c86c893-f48c-4dbf-ae87-fe6f70088fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gacha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f789d7b7-f959-427d-9a4d-fc4f3f37c383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gacha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d851a9cc-2244-4bc0-afdf-68ac04d9ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gacha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe10f86-7e71-49b5-a733-576ad9de72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome, to day one of learning NLP.\n",
    "This is an exciting journey! Stay consistent till I finish.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f40183-1ead-4323-adae-3b2610fdd67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to day one of learning NLP.\n",
      "This is an exciting journey! Stay consistent till I finish.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284ba27d-ad88-4efb-bb3b-faf198342bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a0b165-88fa-42be-bc09-b5830de67657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Welcome, to day one of learning NLP.',\n",
       " 'This is an exciting journey!',\n",
       " 'Stay consistent till I finish.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=sent_tokenize(corpus)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "760bfcc7-de1e-467f-acab-3fc1d004322b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "584c40d8-522d-436c-9b0c-2d9b591b5334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to day one of learning NLP.\n",
      "This is an exciting journey!\n",
      "Stay consistent till I finish.\n"
     ]
    }
   ],
   "source": [
    "# documents[0]\n",
    "for sentense in documents:\n",
    "    print(sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c43ff4f5-a80a-4c00-bbb3-249d867558a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization .converting sentence/paragraph into words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6337acd0-d952-4a6e-8878-9a398b992803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'day',\n",
       " 'one',\n",
       " 'of',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'exciting',\n",
       " 'journey',\n",
       " '!',\n",
       " 'Stay',\n",
       " 'consistent',\n",
       " 'till',\n",
       " 'I',\n",
       " 'finish',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c021152e-5669-400d-b675-cd0806b931ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'day', 'one', 'of', 'learning', 'NLP', '.']\n",
      "['This', 'is', 'an', 'exciting', 'journey', '!']\n",
      "['Stay', 'consistent', 'till', 'I', 'finish', '.']\n"
     ]
    }
   ],
   "source": [
    "# documents[0]\n",
    "for sentense in documents:\n",
    "    print(word_tokenize(sentense))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82f63c40-ced3-4e4b-8c49-d9dde063d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5532df3-8528-46a6-bfbf-ea2a719e2ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'day',\n",
       " 'one',\n",
       " 'of',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'exciting',\n",
       " 'journey',\n",
       " '!',\n",
       " 'Stay',\n",
       " 'consistent',\n",
       " 'till',\n",
       " 'I',\n",
       " 'finish',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25c333ee-08fb-480d-996a-a34ef14a1e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e8f1915-1bb2-4669-940c-6e8199229951",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dec0bdbf-e564-4cc6-be57-0cea3788a75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'day',\n",
       " 'one',\n",
       " 'of',\n",
       " 'learning',\n",
       " 'NLP.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'exciting',\n",
       " 'journey',\n",
       " '!',\n",
       " 'Stay',\n",
       " 'consistent',\n",
       " 'till',\n",
       " 'I',\n",
       " 'finish',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apart form the last fulstop the others are not considered as separate words\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53b5f0-06ff-44bf-9304-2d4fb16d1786",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "is the process of reducing words to its words stem that affixes to suffixes and prefixes to the roots of the words known as a lemma ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94f2ddf9-5182-4ef5-983b-c61a6f65bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Problem \n",
    "words=[\"eating\",\"eaten\",\"eats\",\"writing\",\"writes\",\"programming\",\"programmed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b18ce7cf-e035-46be-9f4d-5c55f83bcd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter stemmer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24de262c-5c7d-403e-91c7-48d6f5b0ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f247de22-dd83-47cf-8b26-a65fb847f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating....-->eat\n",
      "eaten....-->eaten\n",
      "eats....-->eat\n",
      "writing....-->write\n",
      "writes....-->write\n",
      "programming....-->program\n",
      "programmed....-->program\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"....-->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69a82f71-fb43-4bb8-8792-58d231044a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpStemmer class\n",
    "# it takes a single reguar expression and removes any prefix and suffix that matches the espression\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58f022fb-4882-40a5-ad69-2c94ddc4cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer(\"en$|es$|able$|ing$\",min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d883ebc-5646-40bd-b7b6-c800b31c13c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d490d5-eec2-43f0-8ec8-ef922479225e",
   "metadata": {},
   "source": [
    "#### SNOWBALL STEAMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f87f4529-b072-42b3-b0e1-8d68db5660d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "029a53fc-a1a2-4476-83dc-104b9c7d818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc7524f3-1fd3-4a0b-ab65-1846f8e22d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eaten ----> eaten\n",
      "eats ----> eat\n",
      "writing ----> write\n",
      "writes ----> write\n",
      "programming ----> program\n",
      "programmed ----> program\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\" ----> \"+snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6353aaf-0af9-493f-b07e-e2d3653418c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstemmer.stem(\"fairly\"),snowballstemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064dab95-3bba-4d8f-98fc-2525983dce37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b44472c8-4cf9-44f2-96fc-1c45c402fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c671fa42-eda0-46a0-a6dd-f2738b66e2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematizer=WordNetLemmatizer()\n",
    "lematizer.lemmatize(\"goes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ad6ea197-2c1b-4b2c-959c-a35a1406047d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematizer.lemmatize(\"going\",pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c4f42739-f8b5-4ce2-bee8-363a9252d94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eaten ---> eat\n",
      "eats ---> eat\n",
      "writing ---> write\n",
      "writes ---> write\n",
      "programming ---> program\n",
      "programmed ---> program\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' ---> '+lematizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffae713-2105-4272-924d-07b0d4b328c2",
   "metadata": {},
   "source": [
    "### stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aea83baf-5141-4867-a01a-30071bf1de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3474941-e207-42b5-9891-68efddf12295",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"My friends, we live in a time of great challenge and great opportunity. The world is shifting, ideas are rising, and the voices of the unheard are growing louder.\n",
    "But change does not come easily. It demands courage. It demands sacrifice. It demands that we stand together, not as enemies divided by fear, but as people united by hope.\n",
    "We must ask ourselves: What kind of world do we want to build? Will we choose justice over indifference? Will we choose unity over division? Will we choose to rise, even when the weight of history tries to hold us down?\n",
    "The answer, my friends, is in our hands. The power to create, to transform, to uplift—it lives within us. It lives in our words, our actions, our determination to never give up.\n",
    "So I say to you: Let us not wait for change. Let us be the change. Let us rise, let us build, and let us never stop until justice, freedom, and dignity belong to all!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee0e109f-92d6-41f0-b248-7027fb952919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My friends, we live in a time of great challenge and great opportunity. The world is shifting, ideas are rising, and the voices of the unheard are growing louder.\n",
      "But change does not come easily. It demands courage. It demands sacrifice. It demands that we stand together, not as enemies divided by fear, but as people united by hope.\n",
      "We must ask ourselves: What kind of world do we want to build? Will we choose justice over indifference? Will we choose unity over division? Will we choose to rise, even when the weight of history tries to hold us down?\n",
      "The answer, my friends, is in our hands. The power to create, to transform, to uplift—it lives within us. It lives in our words, our actions, our determination to never give up.\n",
      "So I say to you: Let us not wait for change. Let us be the change. Let us rise, let us build, and let us never stop until justice, freedom, and dignity belong to all!\n"
     ]
    }
   ],
   "source": [
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d18669e4-99a8-41a1-8c5e-e0f2328625a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d2dbddc-b328-4fdf-b7b9-0aabba119b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b9c4b1e3-2fbd-4e6a-91e8-fa6eff9c0748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3c1825db-83bb-4499-ab42-60d1275620cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "264d2547-b495-4796-b394-62d5b7938da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d18a443a-1f7d-4515-9347-1fd67742da22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fc1b3b8c-7b48-4027-b4e9-cd6d6d3cc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stop_words)]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e17de46-2a40-4723-aee0-d4dcada446e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my friend , live time great challeng great opportun .',\n",
       " 'the world shift , idea rise , voic unheard grow louder .',\n",
       " 'but chang come easili .',\n",
       " 'it demand courag .',\n",
       " 'it demand sacrific .',\n",
       " 'it demand stand togeth , enemi divid fear , peopl unit hope .',\n",
       " 'we must ask : what kind world want build ?',\n",
       " 'will choos justic indiffer ?',\n",
       " 'will choos uniti divis ?',\n",
       " 'will choos rise , even weight histori tri hold us ?',\n",
       " 'the answer , friend , hand .',\n",
       " 'the power creat , transform , uplift—it live within us .',\n",
       " 'it live word , action , determin never give .',\n",
       " 'so i say : let us wait chang .',\n",
       " 'let us chang .',\n",
       " 'let us rise , let us build , let us never stop justic , freedom , digniti belong !']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1a9272f1-8d1f-4f7c-a3ad-dd7eae16cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[snowballstemmer.stem(word) for word in words if word not in set(stop_words)]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "36f6d255-8d84-4778-9bfd-6a7450f8c72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my friend , live time great challeng great opportun .',\n",
       " 'the world shift , idea rise , voic unheard grow louder .',\n",
       " 'but chang come easili .',\n",
       " 'it demand courag .',\n",
       " 'it demand sacrific .',\n",
       " 'it demand stand togeth , enemi divid fear , peopl unit hope .',\n",
       " 'we must ask : what kind world want build ?',\n",
       " 'will choos justic indiffer ?',\n",
       " 'will choos uniti divis ?',\n",
       " 'will choos rise , even weight histori tri hold us ?',\n",
       " 'the answer , friend , hand .',\n",
       " 'the power creat , transform , uplift—it live within us .',\n",
       " 'it live word , action , determin never give .',\n",
       " 'so i say : let us wait chang .',\n",
       " 'let us chang .',\n",
       " 'let us rise , let us build , let us never stop justic , freedom , digniti belong !']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "623b0ce5-c224-4362-aa0a-99a4c289582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=\"v\",\"n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2d228586-340f-4f4a-9708-3fd970ffdbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematizer.lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4464757c-14b0-4b3a-afb2-fcc2bc00e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lematizer.lemmatize(word.lower(),pos=\"n\") for word in words if word not in set(stop_words)]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "773d473d-bc57-484c-bc3e-2623fabcf4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friend , live time great challenge great opportunity .',\n",
       " 'world shift , idea rise , voice unheard grow louder .',\n",
       " 'change come easily .',\n",
       " 'demand courage .',\n",
       " 'demand sacrifice .',\n",
       " 'demand stand together , enemy divide fear , people unite hope .',\n",
       " 'must ask : kind world want build ?',\n",
       " 'choose justice indifference ?',\n",
       " 'choose unity division ?',\n",
       " 'choose rise , even weight history try hold u ?',\n",
       " 'answer , friend , hand .',\n",
       " 'power create , transform , uplift—it live within u .',\n",
       " 'live word , action , determination never give .',\n",
       " 'say : let u wait change .',\n",
       " 'let u change .',\n",
       " 'let u rise , let u build , let u never stop justice , freedom , dignity belong !']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48762d-012c-4e32-ab2b-0edf56ccbb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
